{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AS2.1 - Model-free prediction & control**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **A. Monte-Carlo policy evaluation**\n",
    "![pseudo code](./montecarlo.png)\n",
    "Bij Monte-Carlo policy evaluation gaan we sampling toepassen in de omgeving op basis van een gegeven policy om de value function te bepalen. Dit doen we door eerst een simulation run te genereren op basis van die policy en alle state transities met de geobserveerde rewards op te slaan. We berekenen ten slotte de verwachte reward voor elke bezochte state. Door dit proces vaak te herhalen convergeren deze verwachte rewards naar de optimale value function. Lees paragraaf 5.1 van Sutton & Barto of bekijk het college van David Silver voor meer informatie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 1**\n",
    "\n",
    "Bepaal handmatig de optimale policy π∗ (deze is eenvoudig visueel af te leiden) en gebruik deze voor de evaluatie. Start elke simulatie op de startpositie in de doolhof. Wat is de value function na een oneindig aantal iteraties als discount γ = 1? Visualiseer dit op een inzichtelijke manier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pseudo code](./policy2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   |      State |  Action |\n",
    "|----------|:-------------:|------:|\n",
    "| 0 |(3,2)|up   |\n",
    "| 1 |(2,2)|left |\n",
    "| 2 |(2,1)|up   |\n",
    "| 3 |(1,1)|up   |\n",
    "| 4 |(0,1)|right|\n",
    "| 5 |(0,2)|right| \n",
    "| 6 |(0,3)|None |      \n",
    "\n",
    "V(s) is weer 0 voor alle states in S\n",
    "\n",
    "Episode:\n",
    "(3,2), up, r=-1\n",
    "down\n",
    "(2,2), left, r=-1\n",
    "down\n",
    "(2,1), up, r=-1\n",
    "down\n",
    "(1,1), up, r=-1\n",
    "down\n",
    "(0,1), right, r=-1\n",
    "down\n",
    "(0,2), right, r=40\n",
    "\n",
    "Now the return (G):\n",
    "G = 0, gamma = 1, Returns(s) = {}\n",
    "\n",
    "Step 1: t=5\n",
    "S_t=(0,2)\n",
    "G = 1 * 0 + 40 = 40\n",
    "Returns(s) = {(0,2): [40]}\n",
    "V(0,2) = 40\n",
    "\n",
    "Step 2: t=4\n",
    "S_t=(0,1)\n",
    "G = 1 * 40 + -1 = 39\n",
    "Returns(s) = {(0,2): [40], (0,1): [39]}\n",
    "V(0,1) = 39\n",
    "\n",
    "Step 3: t=3\n",
    "S_t=(1,1)\n",
    "G = 1 * 39 + -1 = 38\n",
    "Returns(s) = {(0,2): [40], (0,1): [39], (1,1): [38]}\n",
    "V(1,1) = 38\n",
    "\n",
    "Step 4: t=2\n",
    "S_t=(2,1)\n",
    "G = 1 * 38 + -1 = 37\n",
    "Returns(s) = {(0,2): [40], (0,1): [39], (1,1): [38], (2,1): [37]}\n",
    "V(2,1) = 37\n",
    "\n",
    "Step 5: t=1\n",
    "S_t=(2,2)\n",
    "G = 1 * 37 + -1 = 36\n",
    "Returns(s) = {(0,2): [40], (0,1): [39], (1,1): [38], (2,1): [37], (2,2): [36]}\n",
    "V(2,2) = 36\n",
    "\n",
    "Step 6: t=0\n",
    "S_t=(3,2)\n",
    "G = 1 * 36 + -1 = 35\n",
    "Returns(s) = {(0,2): [40], (0,1): [39], (1,1): [38], (2,1): [37], (2,2): [36], (3,2): [35]}\n",
    "V(3,2) = 35\n",
    "\n",
    "\n",
    "Dit is hetzelfde geval als de eerste odpracht, omdat er maar 1 episode is, heeft het geen zin.\n",
    "p.s. it's logical, du\n",
    "| |0|1|2|3|\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|**3**|v=0|v=0|v=35|v=0|\n",
    "|**2**|v=0|v=36|v=37|v=0|\n",
    "|**1**|v=0|v=38|v=0|v=0|\n",
    "|**0**|v=0|v=39|v=40|v=0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 2**\n",
    "Wat is de value function na een oneindig aantal iteraties als γ = 0.5? Visualiseer weer op een inzichtelijke manier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. gamma = 0.5\n",
    "\n",
    "Let's initialize V(s) for all s in S as 0\n",
    "\n",
    "Then generate the only available episode:\n",
    "State: (2,0), Action: Up, Reward: -1\n",
    "State: (2,1), Action: Left, Reward: -1\n",
    "State: (1,1), Action: Up, Reward: -1\n",
    "State: (1,2), Action: Up, Reward: -1\n",
    "State: (1,3), Action: Right, Reward: -1\n",
    "State: (2,3), Action: Right, Reward: 40\n",
    "\n",
    "Now the return (G):\n",
    "G = 0, gamma = 0.5, Returns(s) = {}\n",
    "\n",
    "Stap 1:\n",
    "t=5\n",
    "S_t=(2,3)\n",
    "G = 0.5 * 0 + 40 = 40\n",
    "Returns(s) = {(2,3): [40]}\n",
    "V(2,3) = 40\n",
    "\n",
    "Stap 2:\n",
    "t=4\n",
    "S_t=(1,3)\n",
    "G = 0.5 * 40 + -1 = 19\n",
    "Returns(s) = {(2,3): [40], (1,3): [19]}\n",
    "V(1,3) = 19\n",
    "\n",
    "Stap 3:\n",
    "t=3\n",
    "S_t=(1,2)\n",
    "G = 0.5 * 19 + -1 = 8.5\n",
    "Returns(s) = {(2,3): [40], (1,3): [19], (1,2): [8.5]}\n",
    "V(1,2) = 8.5\n",
    "\n",
    "Stap 4:\n",
    "t=2\n",
    "S_t=(1,1)\n",
    "G = 0.5 * 8.5 + -1 = 3.25\n",
    "Returns(s) = {(2,3): [40], (1,3): [19], (1,2): [8.5], (1,1): [3.25]}\n",
    "V(1,1) = 3.25\n",
    "\n",
    "Stap 5:\n",
    "t=1\n",
    "S_t=(2,1)\n",
    "G = 0.5 * 3.25 + -1 = 0.625\n",
    "Returns(s) = {(2,3): [40], (1,3): [19], (1,2): [8.5], (1,1): [3.25], (2,1): [0.625]}\n",
    "V(2,1) = 0.625\n",
    "\n",
    "Stap 6:\n",
    "t=0\n",
    "S_t=(2,0)\n",
    "G = 0.5 * 0.0625 + -1 = -0.6875\n",
    "Returns(s) = {(2,3): [40], (1,3): [19], (1,2): [8.5], (1,1): [3.25], (2,1): [0.625], (2,0): [-0.6875]}\n",
    "V(2,0) = -0.6875\n",
    "\n",
    "| |0|1|2|3|\n",
    "|:-:|:-:|:-:|:-:|:-:|\n",
    "|**3**|v=0|v=19|v=40|v=0|\n",
    "|**2**|v=0|v=8.5|v=0|v=0|\n",
    "|**1**|v=0|v=3.25|v=0.625|v=0|\n",
    "|**0**|v=0|v=0|v=-0.6875|v=0|\n",
    "\n",
    "Dit is weer hetzelfde geval als de eerste opdracht, gemiddelde van oneindig keer hetzelfde getal blijft hetzelfde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 3**\n",
    "\n",
    "Als het goed is, kom je met deze policy tot een onvolledige value function. Waarom is dat het geval? Hoe kan je dat oplossen?\n",
    "\n",
    "Dit gebeurt omdat je alleen de policy volgt. Dit zorgt ervoor dat je niet alle states bereikt. Dit is op te lossen door aan de hand van de $\\epsilon$, met een kleine kans, een random actie te selecteren in plaats van de policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercise 4**\n",
    "Stel, je maakt gebruik van any-visit Monte-Carlo prediction (in plaats van first-visit). Maakt dat een verschil voor het resultaat? Waarom wel/niet?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Er zal geen verschil zijn tussen de uitkomsten, omdat S nooit dubbel voorkomt in de huidige simulate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **B. Temporal difference learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1**\n",
    "In de pseudocode van temporal difference learning hierboven worden alle values een willekeurige waarde toegewezen, behalve de terminal states -- deze worden geïnitialiseerd op 0. Bij Monte-Carlo policy evaluation was deze uitzondering niet nodig. Waarom niet?\n",
    "\n",
    "geinitialiseerde values boeien niet bij Monte Carlo policy evaluation, omdat deze niet gebruikt worden om de value te berekenen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 2**\n",
    "\n",
    "Noem een voordeel en een nadeel van dit algoritme ten opzichte van het Monte-Carlo algoritme.\n",
    "\n",
    "Dit algoritme is veel eleganter dan Monte Carlo. Waar Monte Carlo redelijk brute force is, is Temporal Difference zeer berekend. Daarom is het sneller en efficiënter.\n",
    "\n",
    "Waar Monte Carlo altijd correcte waarden kan genereren door praktisch een oneindig aantal iteraties te voltooien, kan Temporal Difference fouten creëren die zich kunnen voortplanten naar andere states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **C. On-policy first-visit Monte-Carlo Control**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 1**\n",
    "\n",
    "Leg in eigen woorden uit wat een Q-function is.\n",
    "\n",
    "De q fucntion is de context van de agent. De functie berekent een verwachting van totale reward per state, waardoor de agent geen model van de environment nodig heeft. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 2**\n",
    "\n",
    "Beargumenteer waarom we bij model-free control geen gebruik kunnen maken van een value function.\n",
    "\n",
    "een value function vereist kennis van de wereld. Deze heeft de agent niet in de context van model-free control. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 3**\n",
    "\n",
    "Leg in eigen woorden uit wat 'on-policy' in de naam van het algoritme betekent.\n",
    "\n",
    "Het algoritme maakt gebruik van een vooraf gedefineerde policy, en een kans of deze te volgen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercise 4**\n",
    "\n",
    "Dit algoritme geeft een deterministische policy. Leg uit of dit een nadeel is voor de agent in onze doolhof.\n",
    "\n",
    "Een stochastische policy zal in nadeel zijn van de agent, omdat de agent aan de hand van een determistische policy altijd het beste pad kan belopen. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
